{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
    "\n",
    "\n",
    "In this notebook, we will be covering the steps on how to do **Latent Dirichlet Allocation (LDA)**, which is one of many topic modeling techniques. It was specifically designed for text data. There are also matrix factorization techniques for topic modeling such as Latent Senmantic Indexing(LSI) and Non-Negative Matrix Factorization (NMF) \n",
    "\n",
    "\n",
    "- Input:\n",
    "    To use a topic modeling technique, you need to provide:\n",
    "    + (1) a document-term matrix;\n",
    "    + (2) the number of topics you would like the algorithm to pick up;\n",
    "    + (3) number of iterations.\n",
    "\n",
    "\n",
    "- Gensim will go through the process of finding the best word distribution for each topic and best topic distribution for each document\n",
    "\n",
    "\n",
    "- Output:\n",
    "    + (1) The top words in each topic; \n",
    "    + (2) your job as a human is to interpret the results and see if the mix of words in each topic make sense. \n",
    "    + (3) If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #1 (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abject</th>\n",
       "      <th>able</th>\n",
       "      <th>ablebodied</th>\n",
       "      <th>abortion</th>\n",
       "      <th>...</th>\n",
       "      <th>ze</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "      <th>éclair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 6533 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaah  aaah  aah  abc  abcs  ability  abject  able  ablebodied  \\\n",
       "ali           0     0    0    1     0        0       0     2           0   \n",
       "anthony       0     0    0    0     0        0       0     0           0   \n",
       "bill          1     0    0    0     1        0       0     1           0   \n",
       "dave          0     1    0    0     0        0       0     0           0   \n",
       "jim           0     0    0    0     0        0       0     1           2   \n",
       "joe           0     0    0    0     0        0       0     2           0   \n",
       "john          0     0    0    0     0        0       0     3           0   \n",
       "louis         0     0    3    0     0        0       0     1           0   \n",
       "mike          0     0    0    0     0        0       0     0           0   \n",
       "ricky         0     0    0    0     0        1       1     2           0   \n",
       "\n",
       "         abortion  ...  ze  zealand  zeppelin  zero  zillion  zombie  zombies  \\\n",
       "ali             0  ...   0        0         0     0        0       1        0   \n",
       "anthony         2  ...   0       10         0     0        0       0        0   \n",
       "bill            0  ...   1        0         0     1        1       1        1   \n",
       "dave            0  ...   0        0         0     0        0       0        0   \n",
       "jim             0  ...   0        0         0     0        0       0        0   \n",
       "joe             0  ...   0        0         0     0        0       0        0   \n",
       "john            0  ...   0        0         0     0        0       0        0   \n",
       "louis           0  ...   0        0         0     2        0       0        0   \n",
       "mike            0  ...   0        0         2     1        0       0        0   \n",
       "ricky           0  ...   0        0         0     0        0       0        0   \n",
       "\n",
       "         zoning  zoo  éclair  \n",
       "ali           0    0       0  \n",
       "anthony       0    0       0  \n",
       "bill          1    0       0  \n",
       "dave          0    0       0  \n",
       "jim           0    0       0  \n",
       "joe           0    0       0  \n",
       "john          0    0       1  \n",
       "louis         0    0       0  \n",
       "mike          0    0       0  \n",
       "ricky         0    1       0  \n",
       "\n",
       "[10 rows x 6533 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('./pickle/dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ali</th>\n",
       "      <th>anthony</th>\n",
       "      <th>bill</th>\n",
       "      <th>dave</th>\n",
       "      <th>jim</th>\n",
       "      <th>joe</th>\n",
       "      <th>john</th>\n",
       "      <th>louis</th>\n",
       "      <th>mike</th>\n",
       "      <th>ricky</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaaaah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abc</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abcs</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ali  anthony  bill  dave  jim  joe  john  louis  mike  ricky\n",
       "aaaaah    0        0     1     0    0    0     0      0     0      0\n",
       "aaah      0        0     0     1    0    0     0      0     0      0\n",
       "aah       0        0     0     0    0    0     0      3     0      0\n",
       "abc       1        0     0     0    0    0     0      0     0      0\n",
       "abcs      0        0     1     0    0    0     0      0     0      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()\n",
    "# each row is a document(transcript) and each columns is a term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "\n",
    "# matutils is a module in gensim for 'math utilities' \n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jingchenli/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator CountVectorizer from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(stop_words=frozenset({&#x27;a&#x27;, &#x27;about&#x27;, &#x27;above&#x27;, &#x27;across&#x27;, &#x27;after&#x27;,\n",
       "                                      &#x27;afterwards&#x27;, &#x27;again&#x27;, &#x27;against&#x27;, &#x27;all&#x27;,\n",
       "                                      &#x27;almost&#x27;, &#x27;alone&#x27;, &#x27;along&#x27;, &#x27;already&#x27;,\n",
       "                                      &#x27;also&#x27;, &#x27;although&#x27;, &#x27;always&#x27;, &#x27;am&#x27;,\n",
       "                                      &#x27;among&#x27;, &#x27;amongst&#x27;, &#x27;amoungst&#x27;, &#x27;amount&#x27;,\n",
       "                                      &#x27;an&#x27;, &#x27;and&#x27;, &#x27;another&#x27;, &#x27;any&#x27;, &#x27;anyhow&#x27;,\n",
       "                                      &#x27;anyone&#x27;, &#x27;anything&#x27;, &#x27;anyway&#x27;,\n",
       "                                      &#x27;anywhere&#x27;, ...}))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(stop_words=frozenset({&#x27;a&#x27;, &#x27;about&#x27;, &#x27;above&#x27;, &#x27;across&#x27;, &#x27;after&#x27;,\n",
       "                                      &#x27;afterwards&#x27;, &#x27;again&#x27;, &#x27;against&#x27;, &#x27;all&#x27;,\n",
       "                                      &#x27;almost&#x27;, &#x27;alone&#x27;, &#x27;along&#x27;, &#x27;already&#x27;,\n",
       "                                      &#x27;also&#x27;, &#x27;although&#x27;, &#x27;always&#x27;, &#x27;am&#x27;,\n",
       "                                      &#x27;among&#x27;, &#x27;amongst&#x27;, &#x27;amoungst&#x27;, &#x27;amount&#x27;,\n",
       "                                      &#x27;an&#x27;, &#x27;and&#x27;, &#x27;another&#x27;, &#x27;any&#x27;, &#x27;anyhow&#x27;,\n",
       "                                      &#x27;anyone&#x27;, &#x27;anything&#x27;, &#x27;anyway&#x27;,\n",
       "                                      &#x27;anywhere&#x27;, ...}))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(stop_words=frozenset({'a', 'about', 'above', 'across', 'after',\n",
       "                                      'afterwards', 'again', 'against', 'all',\n",
       "                                      'almost', 'alone', 'along', 'already',\n",
       "                                      'also', 'although', 'always', 'am',\n",
       "                                      'among', 'amongst', 'amoungst', 'amount',\n",
       "                                      'an', 'and', 'another', 'any', 'anyhow',\n",
       "                                      'anyone', 'anything', 'anyway',\n",
       "                                      'anywhere', ...}))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"./pickle/cv_stop.pkl\", \"rb\"))\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3203: 'ladies',\n",
       " 2407: 'gentlemen',\n",
       " 6330: 'welcome',\n",
       " 5448: 'stage',\n",
       " 130: 'ali',\n",
       " 6432: 'wong',\n",
       " 2700: 'hi',\n",
       " 2682: 'hello',\n",
       " 5799: 'thank',\n",
       " 1173: 'coming',\n",
       " 5132: 'shit',\n",
       " 903: 'cause',\n",
       " 4179: 'pee',\n",
       " 3652: 'minutes',\n",
       " 1969: 'everybody',\n",
       " 6081: 'um',\n",
       " 1988: 'exciting',\n",
       " 1474: 'day',\n",
       " 6494: 'year',\n",
       " 6032: 'turned',\n",
       " 6503: 'yes',\n",
       " 236: 'appreciate',\n",
       " 6077: 'uh',\n",
       " 5759: 'tell',\n",
       " 2418: 'getting',\n",
       " 3979: 'older',\n",
       " 2430: 'girl',\n",
       " 353: 'automatic',\n",
       " 5828: 'thought',\n",
       " 2331: 'fuck',\n",
       " 5539: 'straight',\n",
       " 3040: 'jealous',\n",
       " 2273: 'foremost',\n",
       " 3608: 'metabolism',\n",
       " 2433: 'girls',\n",
       " 1829: 'eat',\n",
       " 5226: 'sixpack',\n",
       " 5803: 'thatthat',\n",
       " 480: 'beautiful',\n",
       " 2937: 'inner',\n",
       " 5819: 'thigh',\n",
       " 1084: 'clearance',\n",
       " 2119: 'feet',\n",
       " 5813: 'theres',\n",
       " 2817: 'huge',\n",
       " 2368: 'gap',\n",
       " 3321: 'light',\n",
       " 4385: 'potential',\n",
       " 4589: 'radiating',\n",
       " 5845: 'throughand',\n",
       " 5257: 'sleep',\n",
       " 2946: 'insomnia',\n",
       " 162: 'ambien',\n",
       " 1730: 'download',\n",
       " 3572: 'meditation',\n",
       " 3939: 'oasis',\n",
       " 4319: 'podcast',\n",
       " 826: 'calm',\n",
       " 967: 'chatter',\n",
       " 4702: 'regret',\n",
       " 4753: 'resentment',\n",
       " 2066: 'family',\n",
       " 1125: 'cluttering',\n",
       " 3638: 'mind',\n",
       " 3353: 'lives',\n",
       " 104: 'ahead',\n",
       " 2812: 'hpv',\n",
       " 4168: 'peace',\n",
       " 3879: 'night',\n",
       " 3976: 'ok',\n",
       " 1160: 'come',\n",
       " 2336: 'fucking',\n",
       " 3393: 'loser',\n",
       " 4960: 'says',\n",
       " 3398: 'lot',\n",
       " 3591: 'men',\n",
       " 6102: 'undetectable',\n",
       " 4648: 'really',\n",
       " 2332: 'fucked',\n",
       " 2420: 'ghost',\n",
       " 2942: 'inside',\n",
       " 3593: 'mens',\n",
       " 621: 'bodies',\n",
       " 634: 'boo',\n",
       " 6426: 'womens',\n",
       " 1686: 'doctor',\n",
       " 5895: 'told',\n",
       " 5541: 'strains',\n",
       " 3159: 'kind',\n",
       " 6031: 'turn',\n",
       " 927: 'cervical',\n",
       " 841: 'cancer',\n",
       " 622: 'body',\n",
       " 2653: 'heal',\n",
       " 2687: 'helpful',\n",
       " 448: 'basically',\n",
       " 1603: 'die',\n",
       " 4412: 'presence',\n",
       " 6421: 'wolverine',\n",
       " 562: 'bitches',\n",
       " 3162: 'kindle',\n",
       " 6033: 'turning',\n",
       " 5049: 'selfhelp',\n",
       " 3308: 'library',\n",
       " 2970: 'interested',\n",
       " 639: 'books',\n",
       " 5094: 'shades',\n",
       " 2520: 'grey',\n",
       " 3316: 'lifechanging',\n",
       " 3453: 'magic',\n",
       " 5858: 'tidying',\n",
       " 1508: 'declutter',\n",
       " 2752: 'home',\n",
       " 31: 'achieve',\n",
       " 4020: 'optimum',\n",
       " 3298: 'level',\n",
       " 5605: 'success',\n",
       " 2785: 'horrible',\n",
       " 4207: 'person',\n",
       " 2615: 'happy',\n",
       " 2685: 'help',\n",
       " 5908: 'tony',\n",
       " 4833: 'robbins',\n",
       " 3578: 'mei',\n",
       " 2736: 'hoarding',\n",
       " 4450: 'problem',\n",
       " 2779: 'hoping',\n",
       " 917: 'center',\n",
       " 4451: 'problems',\n",
       " 2457: 'goes',\n",
       " 368: 'away',\n",
       " 1632: 'disappear',\n",
       " 3698: 'mom',\n",
       " 6452: 'world',\n",
       " 1316: 'country',\n",
       " 5732: 'taught',\n",
       " 5846: 'throw',\n",
       " 1598: 'dictators',\n",
       " 4062: 'overtake',\n",
       " 5311: 'snatch',\n",
       " 6302: 'wealth',\n",
       " 522: 'better',\n",
       " 2739: 'hold',\n",
       " 4773: 'retainer',\n",
       " 2483: 'grade',\n",
       " 2601: 'handy',\n",
       " 5164: 'shovel',\n",
       " 798: 'busy',\n",
       " 5593: 'stuffing',\n",
       " 2459: 'gold',\n",
       " 799: 'butt',\n",
       " 4896: 'running',\n",
       " 1187: 'communiststhe',\n",
       " 4924: 'san',\n",
       " 2299: 'francisco',\n",
       " 6016: 'trying',\n",
       " 4802: 'rid',\n",
       " 6460: 'worst',\n",
       " 2013: 'experience',\n",
       " 3315: 'life',\n",
       " 1894: 'emotional',\n",
       " 4998: 'screaming',\n",
       " 2151: 'fighting',\n",
       " 6499: 'yelling',\n",
       " 831: 'came',\n",
       " 1090: 'climax',\n",
       " 4694: 'refused',\n",
       " 3292: 'let',\n",
       " 5794: 'texas',\n",
       " 2960: 'instruments',\n",
       " 3488: 'manual',\n",
       " 818: 'calculator',\n",
       " 4449: 'probably',\n",
       " 418: 'bamboozled',\n",
       " 2396: 'generation',\n",
       " 4748: 'required',\n",
       " 804: 'buy',\n",
       " 1305: 'cost',\n",
       " 3100: 'judy',\n",
       " 3061: 'jetsons',\n",
       " 3221: 'laptop',\n",
       " 2358: 'future',\n",
       " 2504: 'graph',\n",
       " 5782: 'tesla',\n",
       " 3837: 'need',\n",
       " 1079: 'clean',\n",
       " 4456: 'procrastinator',\n",
       " 216: 'anymore',\n",
       " 27: 'according',\n",
       " 1513: 'deepakoprah',\n",
       " 6298: 'way',\n",
       " 2519: 'grew',\n",
       " 4149: 'past',\n",
       " 3508: 'married',\n",
       " 6492: 'yeah',\n",
       " 3473: 'man',\n",
       " 3420: 'lucky',\n",
       " 2554: 'guy',\n",
       " 2384: 'gave',\n",
       " 2276: 'forever',\n",
       " 1462: 'dated',\n",
       " 3394: 'losers',\n",
       " 3399: 'lots',\n",
       " 5232: 'skaters',\n",
       " 6267: 'wanna',\n",
       " 2533: 'grownass',\n",
       " 6422: 'woman',\n",
       " 5528: 'stop',\n",
       " 1465: 'dating',\n",
       " 6112: 'unless',\n",
       " 6253: 'wake',\n",
       " 3544: 'mattress',\n",
       " 3172: 'kitchen',\n",
       " 5817: 'theyre',\n",
       " 5092: 'sexy',\n",
       " 4050: 'outside',\n",
       " 3471: 'malt',\n",
       " 3339: 'liquor',\n",
       " 2840: 'husband',\n",
       " 3607: 'met',\n",
       " 6314: 'wedding',\n",
       " 2696: 'hes',\n",
       " 3382: 'looking',\n",
       " 3258: 'league',\n",
       " 4953: 'saw',\n",
       " 3971: 'oh',\n",
       " 2454: 'god',\n",
       " 5821: 'thing',\n",
       " 3262: 'learned',\n",
       " 333: 'attending',\n",
       " 2629: 'harvard',\n",
       " 795: 'business',\n",
       " 4975: 'school',\n",
       " 5968: 'trap',\n",
       " 293: 'ass',\n",
       " 2458: 'going',\n",
       " 5969: 'trapped',\n",
       " 2934: 'initially',\n",
       " 3171: 'kissing',\n",
       " 2145: 'fifth',\n",
       " 1461: 'date',\n",
       " 6127: 'unusual',\n",
       " 1601: 'did',\n",
       " 4529: 'purpose',\n",
       " 3179: 'knew',\n",
       " 893: 'catch',\n",
       " 2475: 'gotta',\n",
       " 3464: 'make',\n",
       " 1791: 'dude',\n",
       " 501: 'believe',\n",
       " 5024: 'secret',\n",
       " 2371: 'garden',\n",
       " 4506: 'public',\n",
       " 4119: 'park',\n",
       " 2795: 'hosted',\n",
       " 4698: 'reggae',\n",
       " 2133: 'fests',\n",
       " 25: 'accidentally',\n",
       " 2754: 'homeless',\n",
       " 2722: 'hipsters',\n",
       " 5533: 'store',\n",
       " 6142: 'urban',\n",
       " 4045: 'outfitters',\n",
       " 5822: 'things',\n",
       " 1236: 'confusing',\n",
       " 2721: 'hipster',\n",
       " 471: 'beard',\n",
       " 2084: 'fashion',\n",
       " 6276: 'warmth',\n",
       " 2610: 'happened',\n",
       " 3355: 'living',\n",
       " 725: 'broad',\n",
       " 1476: 'daylight',\n",
       " 989: 'chemistry',\n",
       " 2698: 'hey',\n",
       " 6286: 'wassup',\n",
       " 6234: 'volvo',\n",
       " 1771: 'drop',\n",
       " 1772: 'dropped',\n",
       " 2460: 'golden',\n",
       " 2381: 'gate',\n",
       " 6290: 'watched',\n",
       " 4893: 'run',\n",
       " 3625: 'middle',\n",
       " 2322: 'friends',\n",
       " 282: 'asian',\n",
       " 5141: 'shocked',\n",
       " 6156: 'usually',\n",
       " 283: 'asianamerican',\n",
       " 6425: 'women',\n",
       " 6305: 'wear',\n",
       " 3160: 'kinda',\n",
       " 2443: 'glasses',\n",
       " 4015: 'opinions',\n",
       " 6364: 'white',\n",
       " 1792: 'dudes',\n",
       " 3844: 'neighborhood',\n",
       " 3463: 'major',\n",
       " 1056: 'city',\n",
       " 164: 'america',\n",
       " 6507: 'yoko',\n",
       " 4001: 'ono',\n",
       " 2046: 'factory',\n",
       " 6347: 'whats',\n",
       " 6480: 'wrong',\n",
       " 2115: 'feel',\n",
       " 4254: 'picturesque',\n",
       " 6335: 'wes',\n",
       " 177: 'anderson',\n",
       " 3754: 'movie',\n",
       " 5739: 'teach',\n",
       " 1285: 'cool',\n",
       " 5591: 'stuff',\n",
       " 6237: 'voting',\n",
       " 4676: 'recycling',\n",
       " 1675: 'disturbing',\n",
       " 1689: 'documentaries',\n",
       " 2980: 'introduce',\n",
       " 2796: 'hot',\n",
       " 2772: 'hookin',\n",
       " 3559: 'mean',\n",
       " 3466: 'makes',\n",
       " 4394: 'powerful',\n",
       " 1832: 'eats',\n",
       " 4541: 'pussy',\n",
       " 15: 'absorbing',\n",
       " 4446: 'privilege',\n",
       " 1930: 'entitlement',\n",
       " 3705: 'money',\n",
       " 2742: 'hole',\n",
       " 6240: 'vulnerable',\n",
       " 1395: 'crush',\n",
       " 2644: 'head',\n",
       " 3699: 'moment',\n",
       " 3151: 'kill',\n",
       " 684: 'brains',\n",
       " 1153: 'colonize',\n",
       " 1154: 'colonizer',\n",
       " 3186: 'knowbut',\n",
       " 3506: 'marriage',\n",
       " 3874: 'nice',\n",
       " 5342: 'somebody',\n",
       " 4583: 'race',\n",
       " 70: 'advantage',\n",
       " 4588: 'racist',\n",
       " 4956: 'say',\n",
       " 2020: 'explain',\n",
       " 2572: 'halffilipino',\n",
       " 2574: 'halfjapanese',\n",
       " 2570: 'halfchinese',\n",
       " 2577: 'halfvietnamese',\n",
       " 5393: 'spend',\n",
       " 4197: 'percent',\n",
       " 5137: 'shitting',\n",
       " 3192: 'korean',\n",
       " 160: 'amazing',\n",
       " 3408: 'love',\n",
       " 760: 'built',\n",
       " 3189: 'knowmy',\n",
       " 674: 'boyfriend',\n",
       " 1399: 'cuban',\n",
       " 3617: 'mexican',\n",
       " 2555: 'guys',\n",
       " 250: 'arent',\n",
       " 6096: 'underrated',\n",
       " 5087: 'sexiest',\n",
       " 2563: 'hair',\n",
       " 3835: 'neck',\n",
       " 3468: 'making',\n",
       " 1703: 'dolphin',\n",
       " 5304: 'smooth',\n",
       " 5271: 'slip',\n",
       " 5264: 'slide',\n",
       " 571: 'black',\n",
       " 2188: 'fish',\n",
       " 5865: 'tilikum',\n",
       " 482: 'bed',\n",
       " 4006: 'oohwee',\n",
       " 3601: 'mess',\n",
       " 3063: 'jewish',\n",
       " 4677: 'red',\n",
       " 2924: 'inflamed',\n",
       " 286: 'ask',\n",
       " 1995: 'exfoliated',\n",
       " 5888: 'today',\n",
       " 3039: 'jdate',\n",
       " 3379: 'loofah',\n",
       " 5800: 'thanks',\n",
       " 4883: 'rug',\n",
       " 785: 'burn',\n",
       " 360: 'avi',\n",
       " 3958: 'odor',\n",
       " 5291: 'smell',\n",
       " 4766: 'responsibility',\n",
       " 6082: 'umami',\n",
       " 2215: 'flavor',\n",
       " 1167: 'comes',\n",
       " 2329: 'fromi',\n",
       " 6125: 'unspoken',\n",
       " 6098: 'understanding',\n",
       " 2571: 'halffancy',\n",
       " 2575: 'halfjungle',\n",
       " 1608: 'difference',\n",
       " 2072: 'fancy',\n",
       " 284: 'asians',\n",
       " 1017: 'chinese',\n",
       " 3035: 'japanese',\n",
       " 2794: 'host',\n",
       " 3987: 'olympics',\n",
       " 3110: 'jungle',\n",
       " 1650: 'diseases',\n",
       " 1610: 'different',\n",
       " 1825: 'east',\n",
       " 1128: 'coast',\n",
       " 4445: 'private',\n",
       " 4304: 'playing',\n",
       " 3201: 'lacrosse',\n",
       " 3263: 'learning',\n",
       " 3232: 'latin',\n",
       " 994: 'chess',\n",
       " 4884: 'rugby',\n",
       " 2156: 'filipino',\n",
       " 868: 'carlton',\n",
       " 1602: 'didnt',\n",
       " 6211: 'vietnamese',\n",
       " 1464: 'dates',\n",
       " 5909: 'took',\n",
       " 4769: 'restaurant',\n",
       " 6336: 'west',\n",
       " 3391: 'los',\n",
       " 183: 'angeles',\n",
       " 823: 'called',\n",
       " 4229: 'pho',\n",
       " 350: 'authentic',\n",
       " 4635: 'read',\n",
       " 6501: 'yelp',\n",
       " 3926: 'number',\n",
       " 5022: 'second',\n",
       " 455: 'bathroom',\n",
       " 3281: 'legit',\n",
       " 1721: 'double',\n",
       " 5644: 'supply',\n",
       " 1107: 'closet',\n",
       " 2362: 'gallons',\n",
       " 581: 'bleach',\n",
       " 319: 'atm',\n",
       " 3439: 'machine',\n",
       " 2495: 'grandma',\n",
       " 2444: 'glaucoma',\n",
       " 3812: 'napping',\n",
       " 1295: 'corner',\n",
       " 6249: 'wait',\n",
       " 5447: 'staff',\n",
       " 3265: 'leave',\n",
       " 1484: 'deaf',\n",
       " 1895: 'emotionally',\n",
       " 17: 'abused',\n",
       " 5917: 'total',\n",
       " 534: 'big',\n",
       " 2719: 'hippies',\n",
       " 388: 'backpack',\n",
       " 5371: 'southeast',\n",
       " 281: 'asia',\n",
       " 6506: 'yoga',\n",
       " 375: 'ayahuasca',\n",
       " 924: 'ceremonies',\n",
       " 5196: 'silent',\n",
       " 4777: 'retreats',\n",
       " 4165: 'pay',\n",
       " 5175: 'shut',\n",
       " 6320: 'weekend',\n",
       " 2450: 'glutenfree',\n",
       " 3562: 'means',\n",
       " 695: 'bread',\n",
       " 5728: 'tastes',\n",
       " 2312: 'freerange',\n",
       " 998: 'chewbacca',\n",
       " 3288: 'lesbian',\n",
       " 5830: 'thousand',\n",
       " 1436: 'daily',\n",
       " 2138: 'fiber',\n",
       " 5413: 'spoken',\n",
       " 6437: 'word',\n",
       " 4322: 'poetry',\n",
       " 4559: 'queef',\n",
       " 5138: 'shitty',\n",
       " 4321: 'poem',\n",
       " 5646: 'supporting',\n",
       " 813: 'caitlyn',\n",
       " 3046: 'jenner',\n",
       " 2355: 'funny',\n",
       " 2720: 'hippydippy',\n",
       " 1698: 'doing',\n",
       " 2892: 'impression',\n",
       " 5004: 'scrolls',\n",
       " 6259: 'wall',\n",
       " 751: 'buddha',\n",
       " 4262: 'piggy',\n",
       " 427: 'bank',\n",
       " 4258: 'pier',\n",
       " 2888: 'imports',\n",
       " 4495: 'providing',\n",
       " 2464: 'good',\n",
       " 2128: 'feng',\n",
       " 5173: 'shui',\n",
       " 2801: 'house',\n",
       " 6495: 'years',\n",
       " 5315: 'sneaking',\n",
       " 5665: 'suspicion',\n",
       " 4482: 'propose',\n",
       " 4420: 'pressuring',\n",
       " 6242: 'wacky',\n",
       " 2984: 'intuition',\n",
       " 4481: 'proposals',\n",
       " 6441: 'work',\n",
       " 2899: 'incept',\n",
       " 2852: 'idea',\n",
       " 3485: 'mans',\n",
       " 4147: 'passively',\n",
       " 1693: 'doesnt',\n",
       " 3602: 'message',\n",
       " 2032: 'extremely',\n",
       " 96: 'aggressively',\n",
       " 5833: 'threaten',\n",
       " 47: 'actually',\n",
       " 3267: 'leaving',\n",
       " 3978: 'old',\n",
       " 3228: 'late',\n",
       " 3865: 'new',\n",
       " 5467: 'start',\n",
       " 3482: 'manipulation',\n",
       " 1430: 'cycle',\n",
       " 5509: 'stick',\n",
       " 2250: 'focus',\n",
       " 5970: 'trapping',\n",
       " 3799: 'nag',\n",
       " 4054: 'outta',\n",
       " 6300: 'weak',\n",
       " 908: 'caves',\n",
       " 2417: 'gets',\n",
       " 2111: 'fed',\n",
       " 2170: 'fine',\n",
       " 3509: 'marry',\n",
       " 4483: 'proposed',\n",
       " 3380: 'look',\n",
       " 1980: 'exact',\n",
       " 4817: 'ring',\n",
       " 6269: 'wanted',\n",
       " 3546: 'maybe',\n",
       " 4273: 'pinterest',\n",
       " 4083: 'page',\n",
       " 5064: 'sent',\n",
       " 518: 'best',\n",
       " 2319: 'friend',\n",
       " 5056: 'send',\n",
       " 4274: 'pinterested',\n",
       " 1914: 'engaged',\n",
       " 4941: 'saturday',\n",
       " 658: 'bought',\n",
       " 1752: 'dress',\n",
       " 2258: 'following',\n",
       " 6025: 'tuesday',\n",
       " 5993: 'tried',\n",
       " 4639: 'ready',\n",
       " 4822: 'ripe',\n",
       " 4866: 'rotten',\n",
       " 421: 'banana',\n",
       " 5658: 'surprised',\n",
       " 3970: 'offstage',\n",
       " 1203: 'completely',\n",
       " 4666: 'recognize',\n",
       " 4209: 'personality',\n",
       " 5333: 'soft',\n",
       " 3933: 'nurturing',\n",
       " 1705: 'domestic',\n",
       " 6338: 'weve',\n",
       " 3019: 'ive',\n",
       " 4076: 'packed',\n",
       " 3424: 'lunch',\n",
       " 5212: 'single',\n",
       " 2672: 'hed',\n",
       " 1550: 'dependent',\n",
       " 2485: 'graduated',\n",
       " 2113: 'feed',\n",
       " 2467: 'goodness',\n",
       " 2662: 'heart',\n",
       " 2989: 'investment',\n",
       " 2166: 'financial',\n",
       " 4637: 'reading',\n",
       " 637: 'book',\n",
       " 5124: 'sheryl',\n",
       " 4927: 'sandberg',\n",
       " 5125: 'shes',\n",
       " 1281: 'coo',\n",
       " 2041: 'facebook',\n",
       " 6483: 'wrote',\n",
       " 4815: 'riled',\n",
       " 863: 'careers',\n",
       " 5714: 'talking',\n",
       " 934: 'challenge',\n",
       " 5219: 'sit',\n",
       " 5701: 'table',\n",
       " 4825: 'rise',\n",
       " 3259: 'lean',\n",
       " 3312: 'lie',\n",
       " 6268: 'want',\n",
       " 2125: 'feminism',\n",
       " 3069: 'job',\n",
       " 6147: 'used',\n",
       " 5287: 'smart',\n",
       " 1266: 'continue',\n",
       " 1796: 'dumb',\n",
       " 918: 'century',\n",
       " 2543: 'guess',\n",
       " 5484: 'stay',\n",
       " 5306: 'snacks',\n",
       " 6289: 'watch',\n",
       " 1878: 'ellen',\n",
       " 5596: 'stupid',\n",
       " 4640: 'real',\n",
       " 560: 'bitch',\n",
       " 4886: 'ruined',\n",
       " 2007: 'expected',\n",
       " 2658: 'hear',\n",
       " 4238: 'phrase',\n",
       " 1722: 'doubleincome',\n",
       " 2803: 'household',\n",
       " 6136: 'upset',\n",
       " 1180: 'comments',\n",
       " 4022: 'options',\n",
       " 2309: 'free',\n",
       " 6124: 'unscheduled',\n",
       " 6126: 'unsupervised',\n",
       " 2887: 'importantly',\n",
       " 5414: 'sponsored',\n",
       " 5136: 'shittier',\n",
       " 2261: 'food',\n",
       " 1816: 'earn',\n",
       " 3009: 'ita',\n",
       " 6255: 'walk',\n",
       " 5816: 'theyll',\n",
       " 3099: 'judgmental',\n",
       " 2806: 'housewives',\n",
       " 5549: 'street',\n",
       " 2805: 'housewife',\n",
       " 6257: 'walking',\n",
       " 3522: 'massages',\n",
       " 3423: 'lululemon',\n",
       " 4103: 'pants',\n",
       " 2402: 'genius',\n",
       " 4775: 'retiredi',\n",
       " 6476: 'write',\n",
       " 2316: 'fresh',\n",
       " 612: 'boat',\n",
       " 3: 'abc',\n",
       " 2511: 'great',\n",
       " 1338: 'coworkers',\n",
       " 6478: 'writing',\n",
       " 5776: 'terms',\n",
       " 3070: 'jobs',\n",
       " 3967: 'office',\n",
       " 5238: 'skin',\n",
       " 5018: 'seat',\n",
       " 6146: 'use',\n",
       " 5893: 'toilet',\n",
       " 4105: 'paper',\n",
       " 1331: 'cover',\n",
       " 5869: 'times',\n",
       " 4909: 'sadass',\n",
       " 3557: 'meal',\n",
       " 3994: 'oneply',\n",
       " 4531: 'purposely',\n",
       " 1612: 'difficult',\n",
       " 4515: 'pull',\n",
       " 6015: 'try',\n",
       " 4626: 'ration',\n",
       " 1186: 'communist',\n",
       " 1851: 'effective',\n",
       " 1525: 'dehydrates',\n",
       " 6405: 'wiping',\n",
       " 1558: 'desert',\n",
       " 3346: 'literally',\n",
       " 5379: 'spat',\n",
       " 1477: 'days',\n",
       " 97: 'ago',\n",
       " 3436: 'macgyver',\n",
       " 379: 'baby',\n",
       " 6403: 'wipe',\n",
       " 3691: 'moisten',\n",
       " 385: 'backfired',\n",
       " 2173: 'fingers',\n",
       " 727: 'broke',\n",
       " 1615: 'digitally',\n",
       " 5512: 'stimulated',\n",
       " 1711: 'doo',\n",
       " 2174: 'finish',\n",
       " 4900: 'rushed',\n",
       " 4111: 'paranoid',\n",
       " 5144: 'shoes',\n",
       " 6095: 'underneath',\n",
       " 5453: 'stall',\n",
       " 1327: 'courtneys',\n",
       " 3343: 'listening',\n",
       " 6251: 'waiting',\n",
       " 5870: 'timing',\n",
       " 2836: 'hurry',\n",
       " 2118: 'feels',\n",
       " 815: 'caked',\n",
       " 3375: 'long',\n",
       " 1454: 'dare',\n",
       " 4993: 'scratch',\n",
       " 6101: 'underwear',\n",
       " 1906: 'end',\n",
       " 3383: 'looks',\n",
       " 2469: 'goonies',\n",
       " 3764: 'muffle',\n",
       " 6458: 'worry',\n",
       " 6189: 'velocity',\n",
       " 5435: 'squeeze',\n",
       " 977: 'cheeks',\n",
       " 5651: 'sure',\n",
       " 5274: 'slow',\n",
       " 5490: 'steady',\n",
       " 4073: 'pace',\n",
       " 6118: 'unpredictable',\n",
       " 3896: 'noise',\n",
       " 5612: 'suddenly',\n",
       " 1947: 'escapes',\n",
       " 721: 'brings',\n",
       " 1512: 'deep',\n",
       " 5102: 'shame',\n",
       " 600: 'blow',\n",
       " 1833: 'echo',\n",
       " 4787: 'reverberate',\n",
       " 1909: 'ends',\n",
       " 2582: 'hallways',\n",
       " 6292: 'watching',\n",
       " 3861: 'netflix',\n",
       " 2999: 'ipad',\n",
       " 645: 'boring',\n",
       " 4744: 'repressed',\n",
       " 5135: 'shits',\n",
       " 3342: 'listen',\n",
       " 4320: 'podcasts',\n",
       " 4288: 'planet',\n",
       " 6271: 'wantyou',\n",
       " 1671: 'distracting',\n",
       " 3392: 'lose',\n",
       " 4761: 'respect',\n",
       " 2741: 'holds',\n",
       " 5360: 'sort',\n",
       " 1368: 'credence',\n",
       " 2659: 'heard',\n",
       " 3855: 'nerve',\n",
       " 422: 'bananas',\n",
       " 2517: 'green',\n",
       " 411: 'ballet',\n",
       " 2214: 'flats',\n",
       " 2092: 'fatherinlaw',\n",
       " 5221: 'sitdown',\n",
       " 4660: 'recently',\n",
       " 5712: 'talk',\n",
       " 4494: 'provide',\n",
       " 1007: 'children',\n",
       " 4447: 'privileged',\n",
       " 1006: 'childhood',\n",
       " 5760: 'telling',\n",
       " 1274: 'conversation',\n",
       " 5348: 'son',\n",
       " 6097: 'understand',\n",
       " 1817: 'earning',\n",
       " 1027: 'choose',\n",
       " 4768: 'rest',\n",
       " 1033: 'chose',\n",
       " 4471: 'promise',\n",
       " 1815: 'early',\n",
       " 4776: 'retirement',\n",
       " 4914: 'said',\n",
       " 3563: 'meant',\n",
       " 1607: 'dieting',\n",
       " 1831: 'eating',\n",
       " 2318: 'fried',\n",
       " 1001: 'chicken',\n",
       " 2342: 'fulfilling',\n",
       " 1568: 'destiny',\n",
       " 1050: 'circle',\n",
       " 2037: 'eyelashes',\n",
       " 3759: 'mrs',\n",
       " 4079: 'pacman',\n",
       " 3293: 'lets',\n",
       " 4678: 'redecoratei',\n",
       " 1655: 'disgusting',\n",
       " 4213: 'pervert',\n",
       " 2526: 'gross',\n",
       " 2163: 'filthy',\n",
       " 189: 'animal',\n",
       " 5468: 'started',\n",
       " 4363: 'porn',\n",
       " 6514: 'young',\n",
       " 88: 'age',\n",
       " 2612: 'happens',\n",
       " 6522: 'yyou',\n",
       " 5182: 'sicker',\n",
       " 2871: 'images',\n",
       " 1356: 'crave',\n",
       " 2972: 'internet',\n",
       " 6512: 'youi',\n",
       " 2860: 'idiot',\n",
       " 4644: 'realize',\n",
       " 6332: 'went',\n",
       " 1350: 'craigslist',\n",
       " 4377: 'posted',\n",
       " 5874: 'tiny',\n",
       " 2123: 'female',\n",
       " 5035: 'seeking',\n",
       " 171: 'anal',\n",
       " 1352: 'crash',\n",
       " 3469: 'male',\n",
       " 2649: 'heads',\n",
       " 6111: 'universe',\n",
       " 5207: 'simultaneously',\n",
       " 2024: 'explode',\n",
       " 2303: 'freaked',\n",
       " 4966: 'scared',\n",
       " 4087: 'pain',\n",
       " 109: 'aint',\n",
       " 6297: 'wax',\n",
       " 2036: 'eyebrows',\n",
       " 5362: 'sorts',\n",
       " 1361: 'crazy',\n",
       " 1593: 'dick',\n",
       " 1974: 'evil',\n",
       " 5027: 'secrets',\n",
       " 3314: 'lies',\n",
       " 5068: 'sephora',\n",
       " 4543: 'puts',\n",
       " 5824: 'thinking',\n",
       " 1432: 'dad',\n",
       " 1468: 'dave',\n",
       " 1967: 'eventually',\n",
       " 942: 'change',\n",
       " 969: 'cheat',\n",
       " 2971: 'interesting',\n",
       " 2743: 'holes',\n",
       " 6513: 'youll',\n",
       " 5091: 'sexually',\n",
       " 41: 'active',\n",
       " 4770: 'result',\n",
       " 3347: 'little',\n",
       " 559: 'bit',\n",
       " 5556: 'stretched',\n",
       " 2165: 'finally',\n",
       " 2122: 'felt',\n",
       " 940: 'chance',\n",
       " 3454: 'magical',\n",
       " 2077: 'fantasy',\n",
       " 4374: 'possible',\n",
       " 1641: 'discover',\n",
       " 4485: 'prostate',\n",
       " 1243: 'conqueror',\n",
       " 2638: 'havent',\n",
       " 5979: 'treat',\n",
       " 5907: 'tonight',\n",
       " 3349: 'live',\n",
       " 6509: 'yolo',\n",
       " 5312: 'sneak',\n",
       " 4539: 'pushpush',\n",
       " 6037: 'tushtush',\n",
       " 309: 'atari',\n",
       " 4756: 'resistance',\n",
       " 5438: 'squirmy',\n",
       " 6455: 'wormy',\n",
       " 5851: 'thumb',\n",
       " 5604: 'succeed',\n",
       " 2386: 'gay',\n",
       " 2105: 'fear',\n",
       " 6035: 'turns',\n",
       " 1951: 'especially',\n",
       " 3611: 'metamorphosizes',\n",
       " 4306: 'pleasure',\n",
       " 2034: 'eye',\n",
       " 1642: 'discovered',\n",
       " 3889: 'nirvana',\n",
       " 3204: 'lady',\n",
       " 1101: 'clit',\n",
       " 2039: 'eyes',\n",
       " 3389: 'lord',\n",
       " 4816: 'rim',\n",
       " 6104: 'unfortunately',\n",
       " 2307: 'freaky',\n",
       " 287: 'asked',\n",
       " 5378: 'spank',\n",
       " 1692: 'does',\n",
       " 16: 'abuse',\n",
       " 5576: 'strongheaded',\n",
       " 3404: 'loudmouthed',\n",
       " 920: 'ceos',\n",
       " 4869: 'roughed',\n",
       " 6270: 'wants',\n",
       " 1271: 'control',\n",
       " 4827: 'risk',\n",
       " 1024: 'choke',\n",
       " 5877: 'tired',\n",
       " 647: 'boss',\n",
       " 484: 'bedroom',\n",
       " 3735: 'motherfucker',\n",
       " 2879: 'impact',\n",
       " 419: 'ban',\n",
       " 649: 'bossy',\n",
       " 1874: 'elementary',\n",
       " 4977: 'schools',\n",
       " 5088: 'sexist',\n",
       " 676: 'boys',\n",
       " 2954: 'instead',\n",
       " 4959: 'saying',\n",
       " 5648: 'supposed',\n",
       " 1992: 'executive',\n",
       " 3256: 'leadership',\n",
       " 5237: 'skills',\n",
       " 4871: 'roundabout',\n",
       " 1407: 'cunt',\n",
       " 2167: 'financially',\n",
       " 926: 'certain',\n",
       " 4324: 'point',\n",
       " 1319: 'couple',\n",
       " 1169: 'comfortably',\n",
       " 77: 'afford',\n",
       " 5261: 'sliced',\n",
       " 3477: 'mango',\n",
       " 2262: 'foods',\n",
       " 2905: 'income',\n",
       " 680: 'bracket',\n",
       " 5568: 'striving',\n",
       " 6519: 'youve',\n",
       " 3805: 'named',\n",
       " 3891: 'noah',\n",
       " 4654: 'rebecca',\n",
       " 3174: 'kiwi',\n",
       " 1453: 'danielle',\n",
       " 4270: 'pineapple',\n",
       " 7: 'able',\n",
       " 5572: 'stroll',\n",
       " 5186: 'sidewalk',\n",
       " 4557: 'quarter',\n",
       " 4434: 'princess',\n",
       " 6148: 'useful',\n",
       " 73: 'advice',\n",
       " 733: 'brothers',\n",
       " 5218: 'sisters',\n",
       " 4089: 'paintballing',\n",
       " 6210: 'vietnam',\n",
       " 6195: 'veteran',\n",
       " 5078: 'seven',\n",
       " 2568: 'half',\n",
       " 3715: 'months',\n",
       " 4407: 'pregnant',\n",
       " 4622: 'rare',\n",
       " 1170: 'comic',\n",
       " 4200: 'perform',\n",
       " 1171: 'comics',\n",
       " 2395: 'generally',\n",
       " 883: 'case',\n",
       " 6319: 'week',\n",
       " 380: 'babys',\n",
       " 4255: 'piece',\n",
       " 198: 'annoying',\n",
       " 1435: 'dads',\n",
       " 341: 'audience',\n",
       " 2710: 'hilarious',\n",
       " 2858: 'identify',\n",
       " 2063: 'fame',\n",
       " 5683: 'swells',\n",
       " 4711: 'relatable',\n",
       " 5611: 'sudden',\n",
       " 951: 'chapping',\n",
       " 3887: 'nipples',\n",
       " 2114: 'feeding',\n",
       " 6306: 'wearing',\n",
       " 2330: 'frozen',\n",
       " 1590: 'diaper',\n",
       " 3839: 'needs',\n",
       " 5171: 'shredding',\n",
       " 2609: 'happen',\n",
       " 5459: 'standup',\n",
       " 5606: 'successful',\n",
       " 2070: 'famous',\n",
       " 1639: 'discouraged',\n",
       " 2639: 'having',\n",
       " 3147: 'kid',\n",
       " 3209: 'lame',\n",
       " 5485: 'stayathome',\n",
       " 1748: 'dream',\n",
       " 6080: 'ultimate',\n",
       " 6427: 'won',\n",
       " 3185: 'knowanother',\n",
       " 1640: 'discouraging',\n",
       " 5973: 'travel',\n",
       " 325: 'attached',\n",
       " 1606: 'dies',\n",
       " 1521: 'definitely',\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())\n",
    "id2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes (iterations). Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"said\" + 0.007*\"shit\" + 0.006*\"yeah\" + 0.006*\"cause\" + 0.005*\"theyre\" + 0.004*\"oh\" + 0.004*\"say\" + 0.004*\"really\" + 0.004*\"going\" + 0.004*\"theres\"'),\n",
       " (1,\n",
       "  '0.009*\"said\" + 0.008*\"fucking\" + 0.006*\"went\" + 0.006*\"thing\" + 0.006*\"say\" + 0.006*\"didnt\" + 0.005*\"fuck\" + 0.005*\"yeah\" + 0.005*\"shit\" + 0.005*\"good\"')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=30)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall a 'topic' is just a distribution of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"shit\" + 0.000*\"lot\" + 0.000*\"oh\" + 0.000*\"fucking\" + 0.000*\"said\" + 0.000*\"yeah\" + 0.000*\"hes\" + 0.000*\"ok\" + 0.000*\"really\" + 0.000*\"cause\"'),\n",
       " (1,\n",
       "  '0.010*\"said\" + 0.006*\"cause\" + 0.006*\"says\" + 0.005*\"really\" + 0.004*\"oh\" + 0.004*\"yeah\" + 0.004*\"shit\" + 0.004*\"goes\" + 0.004*\"jenny\" + 0.004*\"uh\"'),\n",
       " (2,\n",
       "  '0.008*\"said\" + 0.008*\"fucking\" + 0.007*\"shit\" + 0.006*\"yeah\" + 0.006*\"theyre\" + 0.006*\"say\" + 0.006*\"thing\" + 0.005*\"didnt\" + 0.005*\"went\" + 0.005*\"hes\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"said\" + 0.000*\"yeah\" + 0.000*\"shit\" + 0.000*\"thing\" + 0.000*\"cause\" + 0.000*\"really\" + 0.000*\"fucking\" + 0.000*\"went\" + 0.000*\"come\" + 0.000*\"good\"'),\n",
       " (1,\n",
       "  '0.012*\"said\" + 0.006*\"cause\" + 0.006*\"fucking\" + 0.006*\"good\" + 0.005*\"thing\" + 0.005*\"really\" + 0.005*\"say\" + 0.005*\"didnt\" + 0.005*\"went\" + 0.005*\"day\"'),\n",
       " (2,\n",
       "  '0.013*\"fcking\" + 0.012*\"fck\" + 0.009*\"theyre\" + 0.007*\"shit\" + 0.006*\"theres\" + 0.006*\"man\" + 0.005*\"house\" + 0.005*\"oh\" + 0.005*\"kids\" + 0.004*\"cause\"'),\n",
       " (3,\n",
       "  '0.010*\"yeah\" + 0.009*\"fucking\" + 0.008*\"shit\" + 0.006*\"hes\" + 0.005*\"said\" + 0.005*\"say\" + 0.005*\"theyre\" + 0.005*\"didnt\" + 0.005*\"little\" + 0.005*\"want\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #2 (Nouns Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.). Check out the UPenn tag set: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jingchenli/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "So, I figured, you know, shit will probably be fine. Traffic stop started off on the right foot. \n",
    "The cops came up to the driver’s side.\n",
    "'''\n",
    "\n",
    "tokenized = word_tokenize(text)\n",
    "tags = pos_tag(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('So', 'RB'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('figured', 'VBD'),\n",
       " (',', ','),\n",
       " ('you', 'PRP'),\n",
       " ('know', 'VBP'),\n",
       " (',', ','),\n",
       " ('shit', 'VBP'),\n",
       " ('will', 'MD'),\n",
       " ('probably', 'RB'),\n",
       " ('be', 'VB'),\n",
       " ('fine', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('Traffic', 'JJ'),\n",
       " ('stop', 'NN'),\n",
       " ('started', 'VBD'),\n",
       " ('off', 'RP'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('right', 'JJ'),\n",
       " ('foot', 'NN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('cops', 'NNS'),\n",
       " ('came', 'VBD'),\n",
       " ('up', 'RB'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('driver', 'NN'),\n",
       " ('’', 'NNP'),\n",
       " ('s', 'JJ'),\n",
       " ('side', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies and gentlemen please welcome to the sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank you thank you thank you san francisco th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>all right thank you thank you very much thank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>this is dave he tells dirty jokes for a living...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>ladies and gentlemen please welcome to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies and gentlemen welcome joe rogan  wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>all right petunia wish me luck out there you w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>introfade the music out lets roll hold there l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thank you thanks thank you guys hey se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello hello how you doing great thank you wow ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies and gentlemen please welcome to the sta...\n",
       "anthony  thank you thank you thank you san francisco th...\n",
       "bill      all right thank you thank you very much thank...\n",
       "dave     this is dave he tells dirty jokes for a living...\n",
       "jim         ladies and gentlemen please welcome to the ...\n",
       "joe         ladies and gentlemen welcome joe rogan  wha...\n",
       "john     all right petunia wish me luck out there you w...\n",
       "louis    introfade the music out lets roll hold there l...\n",
       "mike     wow hey thank you thanks thank you guys hey se...\n",
       "ricky    hello hello how you doing great thank you wow ..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('pickle/data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies gentlemen stage ali hi thank hello na s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank thank people i em i francisco city world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>thank thank pleasure georgia area oasis i june...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>jokes living stare work profound train thought...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>ladies gentlemen stage mr jim jefferies thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies gentlemen joe fck thanks phone fckface ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>petunia thats hello hello chicago thank crowd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>music lets lights lights thank i i place place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thanks look insane years everyone i id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello thank fuck thank im gon youre weve money...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies gentlemen stage ali hi thank hello na s...\n",
       "anthony  thank thank people i em i francisco city world...\n",
       "bill     thank thank pleasure georgia area oasis i june...\n",
       "dave     jokes living stare work profound train thought...\n",
       "jim      ladies gentlemen stage mr jim jefferies thank ...\n",
       "joe      ladies gentlemen joe fck thanks phone fckface ...\n",
       "john     petunia thats hello hello chicago thank crowd ...\n",
       "louis    music lets lights lights thank i i place place...\n",
       "mike     wow hey thanks look insane years everyone i id...\n",
       "ricky    hello thank fuck thank im gon youre weve money..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abortions</th>\n",
       "      <th>abuse</th>\n",
       "      <th>accent</th>\n",
       "      <th>accents</th>\n",
       "      <th>access</th>\n",
       "      <th>...</th>\n",
       "      <th>yulin</th>\n",
       "      <th>yummy</th>\n",
       "      <th>ze</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoo</th>\n",
       "      <th>éclair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 4030 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aah  abc  abcs  ability  abortion  abortions  abuse  accent  accents  \\\n",
       "ali        0    1     0        0         0          0      0       0        0   \n",
       "anthony    0    0     0        0         2          0      0       1        0   \n",
       "bill       0    0     1        0         0          0      0       0        0   \n",
       "dave       0    0     0        0         0          1      0       0        0   \n",
       "jim        0    0     0        0         0          0      0       4        0   \n",
       "joe        0    0     0        0         0          0      1       1        1   \n",
       "john       0    0     0        0         0          0      0       1        0   \n",
       "louis      3    0     0        0         0          0      0       0        0   \n",
       "mike       0    0     0        0         0          0      0       0        0   \n",
       "ricky      0    0     0        1         0          0      0       0        0   \n",
       "\n",
       "         access  ...  yulin  yummy  ze  zealand  zeppelin  zillion  zombie  \\\n",
       "ali           0  ...      0      0   0        0         0        0       1   \n",
       "anthony       0  ...      0      0   0       10         0        0       0   \n",
       "bill          0  ...      0      0   1        0         0        1       1   \n",
       "dave          0  ...      0      0   0        0         0        0       0   \n",
       "jim           0  ...      0      0   0        0         0        0       0   \n",
       "joe           0  ...      0      0   0        0         0        0       0   \n",
       "john          2  ...      0      0   0        0         0        0       0   \n",
       "louis         2  ...      0      0   0        0         0        0       0   \n",
       "mike          0  ...      0      0   0        0         2        0       0   \n",
       "ricky         0  ...      1      1   0        0         0        0       0   \n",
       "\n",
       "         zombies  zoo  éclair  \n",
       "ali            0    0       0  \n",
       "anthony        0    0       0  \n",
       "bill           1    0       0  \n",
       "dave           0    0       0  \n",
       "jim            0    0       0  \n",
       "joe            0    0       0  \n",
       "john           0    0       1  \n",
       "louis          0    0       0  \n",
       "mike           0    0       0  \n",
       "ricky          0    1       0  \n",
       "\n",
       "[10 rows x 4030 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['fuck', 'fucking', 'fck', 'fcking', 'shit', 'like', 'im', 'know', 'just', 'dont', \n",
    "                  'thats', 'right', 'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = list(text.ENGLISH_STOP_WORDS.union(add_stop_words))\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names_out())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"people\" + 0.012*\"day\" + 0.009*\"cause\" + 0.008*\"thing\" + 0.007*\"lot\" + 0.006*\"way\" + 0.006*\"women\" + 0.006*\"guy\" + 0.005*\"baby\" + 0.005*\"school\"'),\n",
       " (1,\n",
       "  '0.023*\"people\" + 0.010*\"thing\" + 0.009*\"man\" + 0.009*\"life\" + 0.009*\"hes\" + 0.007*\"gon\" + 0.007*\"guy\" + 0.007*\"kids\" + 0.006*\"way\" + 0.006*\"day\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.017*\"people\" + 0.009*\"cause\" + 0.009*\"life\" + 0.008*\"thing\" + 0.008*\"hes\" + 0.007*\"guy\" + 0.007*\"way\" + 0.007*\"man\" + 0.006*\"lot\" + 0.006*\"gon\"'),\n",
       " (1,\n",
       "  '0.011*\"people\" + 0.009*\"day\" + 0.007*\"thing\" + 0.007*\"hes\" + 0.006*\"joke\" + 0.006*\"years\" + 0.006*\"lot\" + 0.005*\"god\" + 0.005*\"id\" + 0.005*\"woman\"'),\n",
       " (2,\n",
       "  '0.022*\"people\" + 0.012*\"day\" + 0.011*\"thing\" + 0.008*\"man\" + 0.008*\"cause\" + 0.008*\"guy\" + 0.007*\"way\" + 0.007*\"life\" + 0.007*\"gon\" + 0.007*\"house\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"people\" + 0.001*\"thing\" + 0.001*\"day\" + 0.000*\"guy\" + 0.000*\"cause\" + 0.000*\"hes\" + 0.000*\"life\" + 0.000*\"things\" + 0.000*\"lot\" + 0.000*\"years\"'),\n",
       " (1,\n",
       "  '0.035*\"people\" + 0.016*\"life\" + 0.011*\"thing\" + 0.010*\"cause\" + 0.010*\"kids\" + 0.010*\"man\" + 0.009*\"hes\" + 0.008*\"gon\" + 0.008*\"theyre\" + 0.008*\"way\"'),\n",
       " (2,\n",
       "  '0.015*\"people\" + 0.013*\"day\" + 0.010*\"thing\" + 0.010*\"cause\" + 0.008*\"way\" + 0.007*\"guy\" + 0.006*\"things\" + 0.006*\"house\" + 0.006*\"women\" + 0.006*\"school\"'),\n",
       " (3,\n",
       "  '0.017*\"people\" + 0.010*\"man\" + 0.010*\"hes\" + 0.009*\"thing\" + 0.008*\"years\" + 0.007*\"day\" + 0.007*\"gon\" + 0.007*\"guy\" + 0.006*\"woman\" + 0.006*\"way\"'),\n",
       " (4,\n",
       "  '0.012*\"lot\" + 0.010*\"husband\" + 0.008*\"day\" + 0.008*\"women\" + 0.008*\"people\" + 0.006*\"cause\" + 0.006*\"dude\" + 0.005*\"gon\" + 0.005*\"god\" + 0.005*\"theyre\"')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=5, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies gentlemen welcome stage ali wong hi wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank san francisco thank good people surprise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>right thank thank pleasure greater atlanta geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>dirty jokes living stare most hard work profou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>ladies gentlemen welcome stage mr jim jefferie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies gentlemen joe fck san francisco thanks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>right petunia august thats good right hello he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>music lets lights lights thank much i i i nice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thanks hey seattle nice look crazy ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello great thank fuck thank lovely welcome im...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies gentlemen welcome stage ali wong hi wel...\n",
       "anthony  thank san francisco thank good people surprise...\n",
       "bill     right thank thank pleasure greater atlanta geo...\n",
       "dave     dirty jokes living stare most hard work profou...\n",
       "jim      ladies gentlemen welcome stage mr jim jefferie...\n",
       "joe      ladies gentlemen joe fck san francisco thanks ...\n",
       "john     right petunia august thats good right hello he...\n",
       "louis    music lets lights lights thank much i i i nice...\n",
       "mike     wow hey thanks hey seattle nice look crazy ins...\n",
       "ricky    hello great thank fuck thank lovely welcome im..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abject</th>\n",
       "      <th>able</th>\n",
       "      <th>ablebodied</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abortions</th>\n",
       "      <th>...</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yyou</th>\n",
       "      <th>ze</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoo</th>\n",
       "      <th>éclair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 4886 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaah  aah  abc  abcs  ability  abject  able  ablebodied  abortion  \\\n",
       "ali           0    0    1     0        0       0     2           0         0   \n",
       "anthony       0    0    0     0        0       0     0           0         2   \n",
       "bill          1    0    0     1        0       0     1           0         0   \n",
       "dave          0    0    0     0        0       0     0           0         0   \n",
       "jim           0    0    0     0        0       0     1           2         0   \n",
       "joe           0    0    0     0        0       0     2           0         0   \n",
       "john          0    0    0     0        0       0     3           0         0   \n",
       "louis         0    3    0     0        0       0     1           0         0   \n",
       "mike          0    0    0     0        0       0     0           0         0   \n",
       "ricky         0    0    0     0        1       1     2           0         0   \n",
       "\n",
       "         abortions  ...  yummy  yyou  ze  zealand  zeppelin  zillion  zombie  \\\n",
       "ali              0  ...      0     1   0        0         0        0       1   \n",
       "anthony          0  ...      0     0   0       10         0        0       0   \n",
       "bill             0  ...      0     0   1        0         0        1       1   \n",
       "dave             1  ...      0     0   0        0         0        0       0   \n",
       "jim              0  ...      0     0   0        0         0        0       0   \n",
       "joe              0  ...      0     0   0        0         0        0       0   \n",
       "john             0  ...      0     0   0        0         0        0       0   \n",
       "louis            0  ...      0     0   0        0         0        0       0   \n",
       "mike             0  ...      0     0   0        0         2        0       0   \n",
       "ricky            0  ...      1     0   0        0         0        0       0   \n",
       "\n",
       "         zombies  zoo  éclair  \n",
       "ali            0    0       0  \n",
       "anthony        0    0       0  \n",
       "bill           1    0       0  \n",
       "dave           0    0       0  \n",
       "jim            0    0       0  \n",
       "joe            0    0       0  \n",
       "john           0    0       1  \n",
       "louis          0    0       0  \n",
       "mike           0    0       0  \n",
       "ricky          0    1       0  \n",
       "\n",
       "[10 rows x 4886 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names_out())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"dude\" + 0.005*\"everybody\" + 0.005*\"kid\" + 0.004*\"different\" + 0.003*\"water\" + 0.003*\"joke\" + 0.003*\"ahah\" + 0.003*\"men\" + 0.003*\"ok\" + 0.002*\"dead\"'),\n",
       " (1,\n",
       "  '0.004*\"joke\" + 0.004*\"mom\" + 0.004*\"kid\" + 0.004*\"friend\" + 0.004*\"sure\" + 0.004*\"family\" + 0.003*\"parents\" + 0.003*\"okay\" + 0.003*\"clinton\" + 0.003*\"jenny\"')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=30)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"dude\" + 0.006*\"kid\" + 0.005*\"gun\" + 0.005*\"ass\" + 0.004*\"guns\" + 0.004*\"everybody\" + 0.004*\"uh\" + 0.003*\"men\" + 0.003*\"son\" + 0.003*\"husband\"'),\n",
       " (1,\n",
       "  '0.005*\"dude\" + 0.005*\"different\" + 0.005*\"everybody\" + 0.005*\"kid\" + 0.005*\"tit\" + 0.004*\"course\" + 0.004*\"weird\" + 0.004*\"girl\" + 0.004*\"dick\" + 0.003*\"men\"'),\n",
       " (2,\n",
       "  '0.007*\"joke\" + 0.004*\"friend\" + 0.004*\"ahah\" + 0.004*\"mom\" + 0.003*\"parents\" + 0.003*\"kid\" + 0.003*\"jenny\" + 0.003*\"clinton\" + 0.003*\"jokes\" + 0.003*\"family\"')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=30)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.015*\"joke\" + 0.006*\"jokes\" + 0.006*\"anthony\" + 0.005*\"twitter\" + 0.004*\"family\" + 0.004*\"funny\" + 0.004*\"grandma\" + 0.004*\"kid\" + 0.004*\"nuts\" + 0.004*\"dead\"'),\n",
       " (1,\n",
       "  '0.007*\"kid\" + 0.006*\"dude\" + 0.006*\"clinton\" + 0.006*\"wife\" + 0.005*\"mom\" + 0.004*\"cow\" + 0.004*\"everybody\" + 0.004*\"parents\" + 0.004*\"gun\" + 0.003*\"okay\"'),\n",
       " (2,\n",
       "  '0.009*\"guns\" + 0.007*\"ass\" + 0.006*\"girlfriend\" + 0.005*\"men\" + 0.005*\"girl\" + 0.005*\"class\" + 0.005*\"gun\" + 0.005*\"cunt\" + 0.005*\"business\" + 0.005*\"son\"'),\n",
       " (3,\n",
       "  '0.005*\"dude\" + 0.004*\"everybody\" + 0.004*\"different\" + 0.004*\"men\" + 0.004*\"ahah\" + 0.004*\"ok\" + 0.003*\"jenny\" + 0.003*\"girl\" + 0.003*\"kid\" + 0.003*\"sure\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=30)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- To explore the relationship between topics and covariates, STM in R is better than Gensim\n",
    "- For visualization of topic models, see LDAvis in R and pyLDAvis in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
