{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66023e6b",
   "metadata": {},
   "source": [
    "# Sample code of using HKUST Azure OpenAI API with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3075bda",
   "metadata": {},
   "source": [
    "# set up\n",
    "You may follow this website to get the API Key \n",
    "https://digitalhumanities.hkust.edu.hk/tutorials/how-to-use-hkust-azure-openai-api-key-with-python-with-sample-code-and-use-case-examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79afebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e86972a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /Users/jingchenli/opt/anaconda3:\r\n",
      "#\r\n",
      "# Name                    Version                   Build  Channel\r\n",
      "openai                    1.9.0            py39hecd8cb5_0  \r\n"
     ]
    }
   ],
   "source": [
    "!conda list openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1df20cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943c5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = \"https://hkust.azure-api.net\",\n",
    "  api_version = \"2023-05-15\",\n",
    "  api_key = \"<your openai api key>\" #put your api key here\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1269f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8yaymtrROfpnKZcBqd2uoxJuJMfAJ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Computational social science is a interdisciplinary field that involves using computer-based techniques toanalyse social data from a vast array of sources. The field draws on methods from computer science, statistics, applied mathematics and the social sciences to study social phenomena at multiple levels and scales. It seeks to uncover patterns and insights in large social datasets through the development of algorithms and statistical models, often using machine learning and data mining techniques. Computational social scientists may use social media data, online behavior data, mobile phone records, economic transactions, or other digital traces of human activity, in order to gain new insights into human social dynamics and behavior. The field is characterized by an emphasis on data and quantitative analysis, and by a commitment to rigorous research design and replicability.', role='assistant', function_call=None, tool_calls=None))], created=1709452520, model='gpt-35-turbo', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=148, prompt_tokens=25, total_tokens=173))\n"
     ]
    }
   ],
   "source": [
    "# Function\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = 'gpt-35-turbo',\n",
    "    temperature = 1,\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a social science professor\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is computational social science?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# print response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7706624",
   "metadata": {},
   "source": [
    "A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly ¾ of a word (so 100 tokens ~= 75 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21697726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionUsage(completion_tokens=148, prompt_tokens=25, total_tokens=173)\n"
     ]
    }
   ],
   "source": [
    "print(response.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfa531e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function\n",
    "def get_response(message, instruction):\n",
    "    response = client.chat.completions.create(\n",
    "        model = 'gpt-35-turbo',\n",
    "        temperature = 1,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # print token usage\n",
    "    print(response.usage)\n",
    "    # return the response\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6bff9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionUsage(completion_tokens=337, prompt_tokens=31, total_tokens=368)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are various ways in which social science researchers can utilize GPT (Generative Pre-trained Transformer) for their computational research. Here are some possible techniques:\\n\\n1. Text Mining: GPT can be used for text mining tasks like sentiment analysis, topic modeling, and named entity recognition. Social science researchers can extract insights from large volumes of social media, news articles, and other text data and analyze the trends, opinions, and behaviors of individuals and communities.\\n\\n2. Language Translation: Social science researchers can use GPT for cross-linguistic analysis of social media content, interviews, surveys, and other language-based data sources. For instance, GPT can be used to translate tweets in different languages and calculate the sentiment of tweets related to a particular topic across different languages.\\n\\n3. Question Answering: GPT can facilitate the answering of complex social science research questions by providing accurate, contextual responses. For instance, Social science researchers can use GPT to answer complex questions related to public policy, social issues, and socioeconomic trends.\\n\\n4. Data Visualization: Social science researchers can use GPT-generated text to create compelling data visualizations, including word clouds, heatmap, and network diagrams. This technique can help researchers to present their data and findings in a format that is easy to understand and interpret by policymakers and the general public.\\n\\n5. Predictive Analysis: Social science researchers can apply GPT for predictive analysis by forecasting future trends and behaviors based on social media behavior. For instance, social science researchers can use GPT to predict the sentiment of tweets and other social media posts related to a particular topic, allowing them to forecast changes in public opinion about a policy issue or a social phenomenon.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(\"How to utilize GPT for computational social science research?\", \"You are a social science professor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71d7321d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are various ways in which social science researchers can utilize GPT (Generative Pre-trained Transformer) for their computational research. Here are some possible techniques:\n",
      "\n",
      "1. Text Mining: GPT can be used for text mining tasks like sentiment analysis, topic modeling, and named entity recognition. Social science researchers can extract insights from large volumes of social media, news articles, and other text data and analyze the trends, opinions, and behaviors of individuals and communities.\n",
      "\n",
      "2. Language Translation: Social science researchers can use GPT for cross-linguistic analysis of social media content, interviews, surveys, and other language-based data sources. For instance, GPT can be used to translate tweets in different languages and calculate the sentiment of tweets related to a particular topic across different languages.\n",
      "\n",
      "3. Question Answering: GPT can facilitate the answering of complex social science research questions by providing accurate, contextual responses. For instance, Social science researchers can use GPT to answer complex questions related to public policy, social issues, and socioeconomic trends.\n",
      "\n",
      "4. Data Visualization: Social science researchers can use GPT-generated text to create compelling data visualizations, including word clouds, heatmap, and network diagrams. This technique can help researchers to present their data and findings in a format that is easy to understand and interpret by policymakers and the general public.\n",
      "\n",
      "5. Predictive Analysis: Social science researchers can apply GPT for predictive analysis by forecasting future trends and behaviors based on social media behavior. For instance, social science researchers can use GPT to predict the sentiment of tweets and other social media posts related to a particular topic, allowing them to forecast changes in public opinion about a policy issue or a social phenomenon.\n"
     ]
    }
   ],
   "source": [
    "print('There are various ways in which social science researchers can utilize GPT (Generative Pre-trained Transformer) for their computational research. Here are some possible techniques:\\n\\n1. Text Mining: GPT can be used for text mining tasks like sentiment analysis, topic modeling, and named entity recognition. Social science researchers can extract insights from large volumes of social media, news articles, and other text data and analyze the trends, opinions, and behaviors of individuals and communities.\\n\\n2. Language Translation: Social science researchers can use GPT for cross-linguistic analysis of social media content, interviews, surveys, and other language-based data sources. For instance, GPT can be used to translate tweets in different languages and calculate the sentiment of tweets related to a particular topic across different languages.\\n\\n3. Question Answering: GPT can facilitate the answering of complex social science research questions by providing accurate, contextual responses. For instance, Social science researchers can use GPT to answer complex questions related to public policy, social issues, and socioeconomic trends.\\n\\n4. Data Visualization: Social science researchers can use GPT-generated text to create compelling data visualizations, including word clouds, heatmap, and network diagrams. This technique can help researchers to present their data and findings in a format that is easy to understand and interpret by policymakers and the general public.\\n\\n5. Predictive Analysis: Social science researchers can apply GPT for predictive analysis by forecasting future trends and behaviors based on social media behavior. For instance, social science researchers can use GPT to predict the sentiment of tweets and other social media posts related to a particular topic, allowing them to forecast changes in public opinion about a policy issue or a social phenomenon.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3673b1b1",
   "metadata": {},
   "source": [
    "# Name Entity Recognition (NER)\n",
    "\n",
    "Name Entity Recognition (NER) is the process of identifying and extracting named entities from text, such as names of people, locations, etc. To illustrate its application, let’s use the text in this webpage as sample data: https://library.hkust.edu.hk/events/conferences/ai-scholarly-commu-2023/\n",
    "\n",
    "The Library is organizing a Researchers’ Series Symposium. The webpage above contains the information of the speakers and their presentation. Let’s use LLMs to extract only the speaker name, their affiliated University, expertise, and presentation title.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e645b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text copied from https://library.hkust.edu.hk/events/conferences/ai-scholarly-commu-2023/\n",
    "\n",
    "Text = \"\"\"\n",
    "14 November 2023\n",
    "2:30 pm – 4:00 pm\n",
    "Search Engine and Large Language Models – Can they truly change the game?\n",
    "REGISTER\n",
    "Abstract\n",
    "Academic search engines are racing to incorporate the latest advancements brought about by Large Language models (LLMs) in terms of their ability to understand queries, extract information and directly generate answers. The first movers in this space were startup and challengers such as Elicit, Consensus.AI, Scite assistant, Scispace but they have recently been joined by established academic search engine provider like Elsevier’s Scopus and Digital Science’s Dimensions joining the fray with more to come.\n",
    "\n",
    "Using techniques like RAG (Retrieval augmented generation), this first wave of academic search engines hopes to combine search technology with generative AI by grounding the answers generated by LLMs using information context found by search engines, with the hope of reducing hallucinations. But is this enough?\n",
    "\n",
    "Join Aaron as he shares his experience testing and using these tools and his best guess on how these tools might develop in the future and their impact on research writing in the future.\n",
    "\n",
    "About the Speaker\n",
    "\n",
    "Aaron TayMr. Aaron Tay has been an academic librarian for over 10 years in Singapore and has worked in a variety of areas including library discovery, research support & bibliometrics. He is current Lead Data Services at the Singapore Management University Libraries and has been honoured for his contributions to the profession with a few awards including Library Association of Singapore (LAS) Professional Service Award, Congress of Southeast Asian Libraries (CONSAL) award (Silver) and Pacific Rim Research Library Alliance (PRRLA) , Karl Lo award.\n",
    "\n",
    "A past contributor to NMC horizon report (library edition), as well as a founding member of the Initiative for Open Abstracts, he has blended his interest in discovery and the evolving Scholarly ecosystem and has given talks on how AI/ML might change Scholarly communication. More recently, he has contributed to panels and given keynotes on the impact of AI and in particular large language models on academic libraries and institutions at conferences like CILIP, IATUL and more. He has been blogging at MusingsAboutLibrarianship.blogpost.com since 2009.\n",
    "\n",
    " \n",
    "\n",
    "15 November 2023\n",
    "4:00 pm – 5:30 pm\n",
    "Saving Time and Sanity: Using active learning for systematic reviews and meta-analyses\n",
    "REGISTER\n",
    "Abstract\n",
    "Screening thousands of research papers for a systematic review or meta-analysis can be overwhelming. The reality is that there simply isn’t enough time to read every single article.\n",
    "\n",
    "Join Prof. Dr. Rens van de Schoot as he introduces ASReview, a powerful free and open-source software for systematic reviewing, developed by his research team from Utrecht University. Rens will explain how active learning, a machine learning technique, can accelerate the step of manual screening process by saving up to 95% (!) of screening time. ASReview is more than just a tool; it’s a vibrant community of researchers, users, and developers worldwide, contributing to its open-source mission, and Rens will explain how you can join the movement towards fast, open, and transparent systematic reviews.\n",
    "\n",
    "About the Speaker\n",
    "\n",
    "prof rens van de schoot profile photoProf. Dr. Rens van de Schoot works as a full professor for ‘Statistics for Small Data Sets’ at Utrecht University in the Netherlands and as an extra-ordinary professor at North-West University in South Africa. He is also the program director of the research master ‘Methodology and Statistics for the Behavioural, Biomedical, and Social Sciences’. He is known for his many tutorials, checklists, and online (free) course materials in the areas of SEM and Bayesian statistics. Currently, his main research project is the community-driven and fully open-source project ASReview: AI-aided systematic reviewing using Active Learning. \n",
    "\n",
    " \n",
    "\n",
    "22 November 2023\n",
    "10:30 am – 12:00 pm\n",
    "Generative AI for Translational Scholarly Communication\n",
    "REGISTER\n",
    "Abstract\n",
    "Many valuable insights embedded in scientific publications are siloed and rarely translated into results that can directly benefit humans. These research-to-practice gaps impede the diffusion of innovation, undermine evidence-based decision making, and contribute to the disconnect between science and the public. Generative AI systems trained on decades of digitized scholarly publications and other human-produced texts are now capable of generating (mostly) high-quality and (sometimes) trustworthy text, images, and media. Applied in the context of scholarly communication, Generative AI can quickly summarize research findings, generate visual diagrams of scientific content, and simplify technical jargon. In essence, Generative AI has the potential to help tailor language, format, tone, and examples to make research more accessible, understandable, engaging, and useful for different audiences.  \n",
    "\n",
    "In this talk, I’ll discuss some uses of Generative AI in these contexts as well as challenges towards realizing the potential of these models, e.g., how to effectively design generated translational science communication artifacts, incorporate human feedback in the process, and mitigate the generation of harmful, misleading, or false information. Scholarly communication is undergoing a major transformation with the emergence of these new tools. By using them safely, we can help bridge the research-to-practice gap and maximize the impacts of scientific discovery. \n",
    "\n",
    "About the Speaker\n",
    "\n",
    "Dr Lucy Lu Wang profile photoLucy Lu Wang is an Assistant Professor at the University of Washington Information School. Her research focuses on how to build better AI and NLP systems for extracting and understanding information from scientific texts; for example, can we create systems that leverage up-to-date literature to help us make better and more data-driven healthcare decisions, or design document understanding models that can improve the readability of scientific texts for people who are blind and low vision. Lucy’s work on supplement interaction detection, gender trends in academic publishing, COVID-19 datasets, and document understanding has been featured in Geekwire, Boing Boing, Axios, VentureBeat, and the New York Times. Prior to joining the UW, she was a Young Investigator at the Allen Institute for AI, and she received her PhD in Biomedical Informatics and Medical Education from the University of Washington.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85f93f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionUsage(completion_tokens=141, prompt_tokens=1302, total_tokens=1443)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'First Event:\\nSpeaker Name: Aaron Tay\\nUniversity: Singapore Management University Libraries\\nSpecialist: Academic Librarian/Discovery\\nPresentation Title: Search Engine and Large Language Models – Can they truly change the game?\\n\\nSecond Event:\\nSpeaker Name: Prof. Dr. Rens van de Schoot\\nUniversity: Utrecht University\\nSpecialist: Statistics for Small Data Sets\\nPresentation Title: Saving Time and Sanity: Using active learning for systematic reviews and meta-analyses\\n\\nThird Event:\\nSpeaker Name: Dr. Lucy Lu Wang\\nUniversity: University of Washington Information School\\nSpecialist: Building better AI and NLP systems for scientific texts\\nPresentation Title: Generative AI for Translational Scholarly Communication'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"You are a helpful assistant.\"\n",
    "\n",
    "get_response(f\"\"\"\n",
    "             Please extract each of the speaker name, \n",
    "             their corresponding university, their specialist (within 20 words), and presentation title. \n",
    "             Text:{Text}\n",
    "            \"\"\", \n",
    "            instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b4dba18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionUsage(completion_tokens=91, prompt_tokens=1309, total_tokens=1400)\n"
     ]
    }
   ],
   "source": [
    "instruction = \"You are a helpful assistant.\"\n",
    "\n",
    "csv_result = get_response(f\"\"\"\n",
    "             Please extract each of the speaker name, their corresponding university, their specialist (within 20 words), and presentation title. Please return the results in csv format.\n",
    "             Text:{Text}\n",
    "            \"\"\", \n",
    "            instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7212679f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Speaker Name, University, Specialist, Presentation Title\\nAaron Tay, Singapore Management University Libraries, Lead Data Services, Using LLMs in Academic Search Engines\\nProf. Dr. Rens van de Schoot, Utrecht University, Statistics for Small Data Sets, Using Active Learning for Systematic Reviews\\nDr. Lucy Lu Wang, University of Washington Information School, AI and NLP for Scientific Texts, Generative AI for Translational Scholarly Communication'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60f6b421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A csv file is created and saved in the same folder of this notebook.\n"
     ]
    }
   ],
   "source": [
    "# save the result to csv file\n",
    "import csv  \n",
    "with open('outputfile_NER-example.csv', 'w', encoding='UTF8') as f:\n",
    "    f.write(csv_result)    \n",
    "    print(\"A csv file is created and saved in the same folder of this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20068e07",
   "metadata": {},
   "source": [
    "# Sentiment Classification\n",
    "Sentiment Classification is a text analysis technique that aims to determine the sentiment or opinion expressed in a given piece of text. It involves classifying text into different sentiment categories such as positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78a70c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really enjoyed the movie. It was fantastic!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The service at the restaurant was terrible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The customer support was very helpful and resp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The shop was okay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The product I bought was of poor quality.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0      I really enjoyed the movie. It was fantastic!\n",
       "1        The service at the restaurant was terrible.\n",
       "2  The customer support was very helpful and resp...\n",
       "3                                 The shop was okay.\n",
       "4          The product I bought was of poor quality."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "sampledata = {\n",
    "    'Text': [\"I really enjoyed the movie. It was fantastic!\", \n",
    "             \"The service at the restaurant was terrible.\", \n",
    "             \"The customer support was very helpful and responsive.\",\n",
    "             \"The shop was okay.\",\n",
    "             \"The product I bought was of poor quality.\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data=sampledata)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4699f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionUsage(completion_tokens=1, prompt_tokens=62, total_tokens=63)\n",
      "CompletionUsage(completion_tokens=1, prompt_tokens=60, total_tokens=61)\n",
      "CompletionUsage(completion_tokens=1, prompt_tokens=61, total_tokens=62)\n",
      "CompletionUsage(completion_tokens=1, prompt_tokens=57, total_tokens=58)\n",
      "CompletionUsage(completion_tokens=1, prompt_tokens=61, total_tokens=62)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really enjoyed the movie. It was fantastic!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The service at the restaurant was terrible.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The customer support was very helpful and resp...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The shop was okay.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The product I bought was of poor quality.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Sentiment\n",
       "0      I really enjoyed the movie. It was fantastic!  positive\n",
       "1        The service at the restaurant was terrible.  negative\n",
       "2  The customer support was very helpful and resp...  positive\n",
       "3                                 The shop was okay.   neutral\n",
       "4          The product I bought was of poor quality.  negative"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the instruction for sentiment analysis\n",
    "instruction = \"Please analyze the sentiment of the following text. Only use the exact wording 'positive', 'negative', or 'neutral' in your response. Do not say any other irrelevant things, no punctuation.\"\n",
    "\n",
    "# Create a new column for sentiment\n",
    "df['Sentiment'] = \"\"\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get the text from the 'Text' column\n",
    "    text = row['Text']\n",
    "    \n",
    "    # Get the sentiment response using the get_response function\n",
    "    sentiment = get_response(text, instruction)\n",
    "    \n",
    "    # Store the sentiment result in the 'Sentiment' column\n",
    "    df.at[index, 'Sentiment'] = sentiment\n",
    "\n",
    "# display the updated DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab228a3",
   "metadata": {},
   "source": [
    "# Semantic Search\n",
    "Furthermore, LLMs can also extend their classification abilities beyond general sentiment. For example, in addition to determining positive or negative sentiment, LLMs can classify text into different aspects or categories.\n",
    "\n",
    "Let’s use the comments that we received in Library Services Quality Survey 2019 (LibQUAL+®) as example to see if LLMs can classify them into these categories: ‘Services’, ‘Facilities’, ‘Resources’, ‘Activities’ or ‘Cannot be classified’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaecc22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The best place to stay in UST.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our library is the best!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love the ground floor of the library, i thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Our library provide information and resources ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>服務良好、環境非常舒適，可以讓我專心溫習、 資源豐富、職員亦很友善，樂意詳細回答我的問 題，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>图书馆有很好的环境，也会提供各种活动和讲座帮助我们熟悉使用图书馆的资源，喜欢我们的图书馆。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0                     The best place to stay in UST.\n",
       "1                           Our library is the best!\n",
       "2  I love the ground floor of the library, i thin...\n",
       "3  Our library provide information and resources ...\n",
       "4  服務良好、環境非常舒適，可以讓我專心溫習、 資源豐富、職員亦很友善，樂意詳細回答我的問 題，...\n",
       "5      图书馆有很好的环境，也会提供各种活动和讲座帮助我们熟悉使用图书馆的资源，喜欢我们的图书馆。"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the comments that the library received in LibQUAL 2019 as sample data https://library.hkust.edu.hk/about-us/user-engagement/libqual2019/\n",
    "sampledata = {\n",
    "    'Text': [\"The best place to stay in UST.\",\n",
    "            \"Our library is the best!\",\n",
    "            \"I love the ground floor of the library, i think especially in the morning it is so peaceful and calm.\",\n",
    "            \"Our library provide information and resources for us, including materials and many classes about how to use them. I really thanks staffs of library. And further I hope that our library could provide more chance and resource of 3D printer and I really love it.\",\n",
    "            \"服務良好、環境非常舒適，可以讓我專心溫習、 資源豐富、職員亦很友善，樂意詳細回答我的問 題，活動種類繁多，可以讓我找到興趣，又能學習 新的東西。\",\n",
    "            \"图书馆有很好的环境，也会提供各种活动和讲座帮助我们熟悉使用图书馆的资源，喜欢我们的图书馆。\"]\n",
    "            }\n",
    "\n",
    "libqual2019 = pd.DataFrame(data=sampledata)\n",
    "libqual2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55c2015a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionUsage(completion_tokens=1, prompt_tokens=70, total_tokens=71)\n",
      "CompletionUsage(completion_tokens=4, prompt_tokens=112, total_tokens=116)\n",
      "CompletionUsage(completion_tokens=1, prompt_tokens=67, total_tokens=68)\n",
      "CompletionUsage(completion_tokens=4, prompt_tokens=109, total_tokens=113)\n",
      "CompletionUsage(completion_tokens=1, prompt_tokens=83, total_tokens=84)\n",
      "CompletionUsage(completion_tokens=3, prompt_tokens=125, total_tokens=128)\n",
      "CompletionUsage(completion_tokens=1, prompt_tokens=113, total_tokens=114)\n",
      "CompletionUsage(completion_tokens=1, prompt_tokens=155, total_tokens=156)\n",
      "CompletionUsage(completion_tokens=1, prompt_tokens=167, total_tokens=168)\n",
      "CompletionUsage(completion_tokens=7, prompt_tokens=209, total_tokens=216)\n",
      "CompletionUsage(completion_tokens=1, prompt_tokens=118, total_tokens=119)\n",
      "CompletionUsage(completion_tokens=5, prompt_tokens=160, total_tokens=165)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The best place to stay in UST.</td>\n",
       "      <td>positive</td>\n",
       "      <td>Cannot be classified.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our library is the best!</td>\n",
       "      <td>positive</td>\n",
       "      <td>Cannot be classified.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love the ground floor of the library, i thin...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Facilities.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Our library provide information and resources ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Resources</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>服務良好、環境非常舒適，可以讓我專心溫習、 資源豐富、職員亦很友善，樂意詳細回答我的問 題，...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Services, Facilities, Resources, Activities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>图书馆有很好的环境，也会提供各种活动和讲座帮助我们熟悉使用图书馆的资源，喜欢我们的图书馆。</td>\n",
       "      <td>positive</td>\n",
       "      <td>Activities, Resources, Services</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Sentiment  \\\n",
       "0                     The best place to stay in UST.  positive   \n",
       "1                           Our library is the best!  positive   \n",
       "2  I love the ground floor of the library, i thin...  positive   \n",
       "3  Our library provide information and resources ...  positive   \n",
       "4  服務良好、環境非常舒適，可以讓我專心溫習、 資源豐富、職員亦很友善，樂意詳細回答我的問 題，...  positive   \n",
       "5      图书馆有很好的环境，也会提供各种活动和讲座帮助我们熟悉使用图书馆的资源，喜欢我们的图书馆。  positive   \n",
       "\n",
       "                                      Category  \n",
       "0                        Cannot be classified.  \n",
       "1                        Cannot be classified.  \n",
       "2                                  Facilities.  \n",
       "3                                    Resources  \n",
       "4  Services, Facilities, Resources, Activities  \n",
       "5              Activities, Resources, Services  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the instruction for sentiment analysis\n",
    "instruction1 = \"Please analyze the sentiment of the following text. Only use the exact wording 'positive', 'negative', or 'neutral' in your response. All lowercase. Do not say any other irrelevant things. Do not include full stop in your response.\"\n",
    "\n",
    "# Define the instruction for category classification\n",
    "instruction2 = \"\"\"\n",
    "Please classify the following text into these categories (exact wordings) 'Services', 'Facilities', 'Resources', 'Activities' or 'Cannot be classified' in your response. Do not say any other words. Remember only use these 4 categories in your response: 'Services', 'Facilities', 'Resources', 'Activities', 'Cannot be classified'. Use 'Cannot be classified' only when no categories can be assigned to the text.\n",
    "\"\"\"\n",
    "\n",
    "# Create new columns for sentiment and category\n",
    "libqual2019['Sentiment'] = \"\"\n",
    "libqual2019['Category'] = \"\"\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in libqual2019.iterrows():\n",
    "    # Get the text from the 'Text' column\n",
    "    text = row['Text']\n",
    "    \n",
    "    # Get the response using the get_response function\n",
    "    sentiment = get_response(text, instruction1)\n",
    "    category = get_response(text, instruction2)\n",
    "    \n",
    "    # Store the result in the column\n",
    "    libqual2019.at[index, 'Sentiment'] = sentiment\n",
    "    libqual2019.at[index, 'Category'] = category\n",
    "\n",
    "# display the updated DataFrame\n",
    "libqual2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14af81f",
   "metadata": {},
   "source": [
    "# Language Translation\n",
    "LLMs can also be effectively utilized for language translation tasks, including the conversion of classical Chinese (文言文) to vernacular Chinese (白話文). \n",
    "\n",
    "Many classical texts, such as ancient philosophical works, historical records and literary masterpieces, hold significant cultural and intellectual importance. However, their language structures and expressions can make them challenging to comprehend for contemporary readers. By employing LLMs for translation, these texts can be rendered into vernacular language, making them more accessible and easier to understand for a wider audience.\n",
    "\n",
    "Let’s use some of the text in《九章算術》(The Nine Chapters on the Mathematical Art) as example. This book is one of the earliest Chinese mathematics books, written during the period from the 10th to the 2nd century BCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dc4aa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionUsage(completion_tokens=207, prompt_tokens=191, total_tokens=398)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'白話文：\\n現在有片田地，長十五步，寬十六步。問這片田有多大？\\n答案是：一畝。\\n另外還有片長十二步，寬十四步的田地。問這田有多大？\\n答案是：一百六十八步。\\n算田地大小的方法是：田地的長和寬相乘，得到田地的面積。用畝（一種面積單位）的標準，每畝是二百四十步。所以，將田地的面積用 240 整除，就能得到這片田有多少畝，一百畝則為一個頃。'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"You are an expert in Mathematics and proficient in classical Chinese.\"\n",
    "Text = \"\"\"\n",
    "今有田廣十五步，從十六步。問為田幾何？\t\t\n",
    "答曰：一畝。\n",
    "方田：又有田廣十二步，從十四步。問為田幾何？\t\t\n",
    "答曰：一百六十八步。\t\t\n",
    "方田術曰：廣從步數相乘得積步。以畝法二百四十步除之，即畝數。百畝為一頃。\n",
    "\"\"\"\n",
    "\n",
    "get_response(f\"\"\"\n",
    "             請把以下文言文翻譯成白話文：\n",
    "             文言文:{Text}\n",
    "            \"\"\", \n",
    "            instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0eff46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionUsage(completion_tokens=316, prompt_tokens=201, total_tokens=517)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'根據題目所提供的數據，田地的長度為15步，寬度為16步。問這片田地的面積是多少？\\n\\n答案是：一畝。\\n\\n對於另一塊方形的田地，長度為12步，寬度為14步。問這片田地的面積是多少？\\n\\n答案是：一百六十八步。\\n\\n根據方田術，田地的面積可以通過將田地的長度和寬度相乘得到。如果要將結果轉換為畝，需要將結果除以240步。一頃等於100畝。\\n\\nTranslation:\\n\\nGiven a field that is 15 steps in length and 16 steps in width, what is its area?\\n\\nThe answer is one mu.\\n\\nFor another square-shaped field that is 12 steps in length and 14 steps in width, what is its area?\\n\\nThe answer is 168 steps.\\n\\nAccording to the method of square fields, the area of a field is obtained by multiplying its length and width. To convert it to mu, the result should be divided by 240 steps. One hectare is equivalent to 100 mu.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"You are an expert in Mathematics and proficient in classical Chinese.\"\n",
    "Text = \"\"\"\n",
    "今有田廣十五步，從十六步。問為田幾何？\t\t\n",
    "答曰：一畝。\n",
    "方田：又有田廣十二步，從十四步。問為田幾何？\t\t\n",
    "答曰：一百六十八步。\t\t\n",
    "方田術曰：廣從步數相乘得積步。以畝法二百四十步除之，即畝數。百畝為一頃。\n",
    "\"\"\"\n",
    "\n",
    "get_response(f\"\"\"\n",
    "             請以數學公式逐步解釋以下文言文的內容, and translate to English:\n",
    "             文言文:{Text}\n",
    "            \"\"\", \n",
    "            instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6482609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5171fab",
   "metadata": {},
   "source": [
    "The tutorial is from\n",
    "https://digitalhumanities.hkust.edu.hk/tutorials/how-to-use-hkust-azure-openai-api-key-with-python-with-sample-code-and-use-case-examples/ \n",
    "\n",
    "Other useful Resources:\n",
    "https://www.deeplearning.ai/short-courses/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
